%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Simple Sectioned Essay Template
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Note:
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article} % Default font size is 12pt, it can be changed here
\usepackage[T1]{fontenc}
\usepackage{geometry} % Required to change the page size to A4
\geometry{letterpaper} % Set the page size to be A4 as opposed to the default US Letter

\usepackage{graphicx} % Required for including pictures
\usepackage{setspace}
\usepackage{abstract}
\usepackage{apacite}
\usepackage{url}
\usepackage{float} % Allows putting an [H] in \begin{figure} to specify the exact location of the figure
\usepackage{wrapfig} % Allows in-line images such as the example fish picture

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{fancyvrb}
\usepackage{underscore}

\linespread{1.2} % Line spacing

%\setlength\parindent{0pt} % Uncomment to remove all indentation from paragraphs

\graphicspath{{./Pictures/}} % Specifies the directory where pictures are stored

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage}
\newgeometry{top=1in,bottom=1in}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page

\includegraphics{logoblack}\\[1cm] % Include a department/university logo - this will require the graphicx package

%\textsc{\LARGE University of Regina}\\[1.5cm] % Name of your university/college
\textsc{\Large Software Systems Engineering}\\[0.5cm] % Major heading such as course name
\textsc{\large ENSE 480}\\[0.5cm] % Minor heading such as course title

\HRule \\[0.4cm]
{ \huge \bfseries Applied Artificial Intelligence}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Julien \textsc{Popa-Liesz} % Your name
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Student ID:} \\
200285869 %\textsc{Smith} % Supervisor's Name
\end{flushright}
\end{minipage}\\[4cm]

{\large \today}\\[3cm] % Date, change the \today to a set date if you want to be precise



\vfill % Fill the rest of the page with whitespace

\end{titlepage}




%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------
\newgeometry{top=1.25in,bottom=1.25in}
\addcontentsline{toc}{section}{Contents}

\pagenumbering{roman}
\tableofcontents % Include a table of contents

\newpage % Begins the essay on a new page instead of on the same page as the table of contents 

%----------------------------------------------------------------------------------------
%	3-D Facial Landmark Localization 
%----------------------------------------------------------------------------------------
\doublespacing
\pagenumbering{arabic}
\section{3-D Facial Landmark Localisation} % Major section

This topic explores a method for automatic localization of facial landmarks where an algorithm generates a set of candidate locations from feature detectors and performs a combinatorial search constrained by a flexible shape model. The application for this research are in the field of biometric identification and medicine. In the field of medicine such applications are facial surgery \cite{Sharifi}, lip movement assessment \cite{Popat} or cranial facial dysmorphology. It is stated in the paper that with the increased use of 3-D scanners the possibility to overcome traditional limitations of 2-D scanners have become discovered. However according to Phillips current 3-D scanners suffer from two shortcomings: 1) not all 3-D scanners provide texture and, even when they do it cannot be assured that it is accurately registered to the geometry. 2) they may become more sensitive to view point and lighting conditions, where texture information is not invariant to these factors. \cite{Phillips} In the presence of obstacles obstructing landmarks, for example, scarf or jacket in the way of a chin the use of shape regression with incomplete local features (SRILF) was used for the detection of such facial landmarks. SRILF handles any combination of missing points and allows for nonrigid deformations. SRILF algorithm has three components: 1) selection of candidates through local feature detection; 2) partial set matching to infer missing landmarks by regression; and 3) combinatorial search which integrates the other two components. Local feature detection is described by a set of vertices in M where M is a facial surface described by the V vertices. Then a set of 3-D annotations containing L landmarks are used to train a local descriptor model for each landmark. The goal is to compute a similarity score s(v) based on the local descriptors, that correlates well with the correct position of the targeted landmark. The second step is Partial set matching using statistical shape models. A shape vector is constructed by concatenating the coordinates of L landmarks. The goal is to infer the coordinates of landmarks using principle component analysis over a FRGC (Facial Recognition Grand Challenge) training set. Combinational feature matching and the use of RANSAC is a matching procedure where L sets of candidate points for each landmark have a set of top scoring vertices which were determined during training period and then evaluated. The SRILF algorithm is described in the paper as follows: Start with the mesh M, for all the landmarks compute the descriptor scores, determine the landmark candidates. Next for all combinations of candidates obtain the best candidates within M. Keep iterating the subset until it achieves the desired highest score and then evaluate the geometry. In conclusion, the algorithm generates sets of candidate points from geometric cues extracted by using the APSC descriptors and then performs a combinatorial search using a constrained flexible shape model. In the scenario were a landmark was not accurately detectable the use of partial subsets of landmarks were taken to make a inference on the missing landmark. After Evaluating the facial recognition grand challenge data, errors in the ranges of approximately 3.5 mm was obtained. Such an accuracy is amazing. It is apparent that the trained data sets using artificial intelligent techniques was used but the type of technique was not stated in the paper. However in this example, applied artificial intelligence was definitely applicable in transactions systems of facial recognition.

\newpage

%----------------------------------------------------------------------------------------
% Differential Evolution with an Evolution Path: A Deep Evolutionary Algorithm
%----------------------------------------------------------------------------------------

\section{Filtering Reviews by Random Individual Error} % Major section
This paper tackles the problem of physician ratings websites by determining the quality of information extracted by applying artificial intelligence to detect the random individual error within the reviews. The paper goes on to describe the challenges as (1) the content and sentiment analysis of the review texts and (2) the removal of the random individual error contained in them \cite{Geierhos}. To tackle these problems these researchers assigned polarities to automatically recognise opinion phrases in reviews and then check for any divergence in the rating and text. Application of this research paper aids in the current increase in fraudulent internet reviews. This is a big issue particularly in online retail, and has attracted attention in recent years from businesses and research community \cite{Mukherjee}. In some cases reviews were biased and or genuine and may appear to be deceptive which made detection of such reviews difficult in the paper. The data set used was a set if ready-made corpora physician reviews from sites such as jamede.de and docinsider.de. The paper then describes how they detect the random individual errors in the reviews by "divergent polarities" broken into 5 categories assurance, reliability, responsiveness,tangibility and time. For example some polarities can be deduced in a rate my professor situation, where a student will use words such as bad, good or incompetent too much work etc. These words can be correlated as divergent polarities. However some contradictions may occur when the review may be overall satisfactory and based on past experiences and or trust. Machine learning cannot be applied to this scenario but the approach of the application of A.I in this paper is data processing of language and error calculation from the qualitative part in the written review and the quantitative part of the actual valued result. When performing the calculations the 5 categories are broken into subcategories for each to determine a probability of individual inconsistency in the data sets obtained from the review. Assurance is broken down into trustworthiness, kindness, consideration. Reliability can be broken down into health, education. Responsiveness is sub-categorised into waiting time. Tangibility would have a subcategory of entertainment, and time would be the time taken. In the paper they calculated the divergence of these categories by determining the arithmetic mean for each category based each random individual error per sub category. This is evaluated rate of individual error in reviews depended on the grammar annotated by the person doing the review. Training sets were generated based on this data where they compared true positives as correct and false positive erroneous reviews. They calculated the precision in ratio between true positive and false positive reviews to generate a score to compare with physician reviews. As a result the average precision of each category  was 8 percent for assurance, 3 percent for reliability, 50 percent for responsiveness, 28 percent for tangibility and 14 percent for time. What was interesting was the wait time and trust evaluation results were show to be the lowest which makes sense as the data obtained from the review would be difficult to infer from individual error correlated from grammar based analysis.

\newpage

%------------------------------------------------

%----------------------------------------------------------------------------------------
%	The Regulation of Steam Pressure in a Drum Boiler by Neural Network and System Identification Technique 
%----------------------------------------------------------------------------------------
\section{The Regulation of Steam Pressure in a Drum Boiler by Neural Network and System Identification Technique} % Sub-section
The application in Artificial intelligence of neural networks is applied to a complex drum boiler system in an attempt to control the output of the system in terms of steam pressure. The inputs of the system are defined as feed water flow rate with respect to heat in order to maintain a desired steam pressure inside the parameters of the boiler. In the paper the ANN method used is a training method based on Levenberg-Marquardt back propagation where the optimal model is obtained by determining the output error using a mean square when the results are filtered. With respect to ANN being applied in similar situations of a complex dynamic system where \cite{Vasickaninova} used a ANN to evaluate an optimal input for a heat exchanger and compared to a PID controller. The purpose of the Levenberg-Marquardt back-propagation was to account for the non linearity of the system associated with differential evolutions. The back propagation of this type was explored by \cite{wu} as means to determine the output of a speaker system. All these models have a relationship between input and their respective outputs based on some type of parameters that can be adjusted by a neural network. The model of this experiment is defined by the inputs and outputs of a dynamic system in a differential equation. The ANN algorithm is used solve this boiler non-linear problem by fitting the regression line using a sum of squares error derivation. The neural network ran for

\newpage
%----------------------------------------------------------------------------------------
%	Music Emotion Recognition With Standard And Melodic Audio Features
%----------------------------------------------------------------------------------------
\section{Music Emotion Recognition With Standard And Melodic Audio Features} % Sub-section
This paper explores the influence of music in the emotions of humans and how this can be studied through datasets of melodies from several songs in an attempt to research a way in searching digital music in a more advanced, flexible way. The purpose of this project is the need for a new technological innovation for the digital era of music. This is due to the increase in demand for more powerful methods of retrieving songs in the context defined by the user. There is several papers related to emotional analysis but it is apparent in this papers perspective that this idea of understanding the emotions invoked from music is a interesting approach in searching for music. There is a current standard for how music is retrieved known as "Music Information Retrieval Evaluation eXchange (MIREX) where the best algorithm used was based on system called a "mood task" were a dataset was organised into 5 types of emotional categories and obtained a search accuracy of 67.8 percent. The paper goes on to purpose an alternative approach to this model called (MER) music emotion recognition in audio by combining both the standard and melodic audio features in music. There are several models that classify emotional states in dimensions, in particular \cite{Russell} model is the most adopted model where there are known from the basis of anger, disgust, fear, happiness, sadness and surprise. The method used for the data acquisition in this paper was to use the MIREX mood classification task to model the emotions in 5 types of clusters. Cluster 1: passionate, rousing, confident, boisterous rowdy; Cluster 2: rollicking, cheerful, fun, sweet, aim able/good natured; Cluster 3: literate, poignant, wistful, bitter sweet, autumnal, brooding; Cluster 4: humorous, silly, campy, quirky, whimsical, witty, wry; Cluster 5: aggressive, fiery, tense/anxious, intense volatile, visceral. Ultimately this data would be trained to read the wave forms of the songs and perform further emotional analysis to better classify where the songs fit in the 5 clusters. This was done through several parameters listed as, timing, dynamics, timbre, articulation, interval, melody, harmony, tonality, rhythm, mode, loudness, musical form and vibrato. The paper goes to state that these parameters are often difficult to extract from audio signals. The use of machine learning was applied in MER under supervised learning. The goal was to supervise the machine to predict which class the test sample of the song belonged to based on previous models. This models used to train was based on ones that were extracted from a ALLMusic database and were fed into the learning machine to determine it's class. The results showed that the feature vibrato had the highest recognition accuracy in search results of 57.6 percent. The key is that the approach for emotion classification in music based on the combination of both standard and melodic audio features is feasible with machine learning. With a bigger dataset and more resources I could see this perspective on search to be an applicable way for tagging music by the emotions is can invoke.
\newpage
%----------------------------------------------------------------------------------------
% Evolving A Team In A First-Person Shooter Game By Using A Genetic Algorithm
%----------------------------------------------------------------------------------------
\section{Evolving A Team In A First-Person Shooter Game By Using A Genetic Algorithm} % Sub-section
This paper explores the application of artificial intelligence in video games, in particular the first person shooter Quake III arena which was released in 1999 and its source code was open to viewing in 2005. The area of A.I applied in this field was through the concepts of genetic algorithm. Applied Genetic programming has been applied to finding winning strategies for games such as backgammon, chess and robocode \cite{Sipper}. The use of AI has been implemented even before the popularity of video games in today’s world. With the demand of new technologies to engage the players, game designers always try to tackle new and smarter ways of designing game agents in their video games to challenge the player in a new way. The use of a genetic algorithm was applied to a team based game in Quake III known as capture the flag. The objective in the game is to capture the flag and return it to your base. You score when the enemy’s flag is successfully return to your base. Each team consists of 5 players where they collect weapons around their environment and try to kill each other and work together to get the flag which initially is located in the centre of the map. The goal of the genetic algorithm is to evolve a computer generated team to play the game to find the optimal solution given a scenario and try defeat the opponent team. The way the game was modelled for the genetic algorithm was by the use of a finite state machine. Where transitions between states would determine a fitness score to be correlated by the genetic algorithm in order to score it self on how well it approaches a solution for a given situation. The paper defines these states as, flag at centre, flag captured by opponent, flag dropped, flag captured by team and scored (by team or opponent). For positive state transitions the fitness score would increase and for a negative transition the fitness score would decrease respectively. The idea was to generate several teams and watch them evolve to a solution. The teams that had the highest fitness score would ultimately be allowed to breed offspring. The offspring would then inherit the parent process and mutate. This would then iterate until the team would have evolved and converged to a optimal solution. The proposed strategy was implemented and test results were obtained. The game was set with a match time of 30 minutes in which each team would play against a computer-controlled team for 400 generations at most. The data found that the fitness values versus generations for various states in the evolution would increase with the number of generations in all states. The study provided an interesting approach using a genetic algorithm to allow a computer team to win in a complex first person shooter environment. With such a wide search space the data was shown to converge to a solution dependant on the amount of generations. In the future with higher computing power we should expect  to have more generations to increase a fitness score in a complex game environment. This will result in very intelligent game agents for users to play against. 

\newpage

%----------------------------------------------------------------------------------------
%IDENTIFYING EVENTS TAKING PLACE IN SECOND LIFE VIRTUAL ENVIRONMENTS
%----------------------------------------------------------------------------------------
\section{Identifying Events Taking Place In Second Life Virtual Environments} % Sub-section
The use of multi-purpose online virtual worlds is the future of human social interactions. With the increase in technology the use of virtual realities have been an outlet for most people. Second life is virtual environment developed by Linden lab. Second life is a sim based virtual reality video game that tries to emulate certain aspects of the actual world in a digital environment. The second life architecture is based on a client/server model which carries out the calculations. There are many regions in the virtual world the user can explore. The scope of the paper is to explore the data set that second life could potentially provide in the research area of artificial intelligence. The benefits that second life can provide for AI application is the use of dynamic events that can interact with the users. The huge environment of second life allows freedom for users to work together and organise together to accomplish common goals with each other. In this respect, it is useful to have a mechanism to identify events taking place in these virtual communities in a social setting. Identifying these events taking place in the virtual training sims is useful for identifying how effective the simulation was in relation to the participates in the simulation \cite{Cranfield}. Cranfield highlights the importance of exploring the uses of A.I application inside second life to help enhance the realism of the virtual reality by creating intelligent agents inside the system to interact with the environment and with the users. This theory would fundamentally improve virtual reality in the future and provide users with a surreal experience. The data set extracted in second life would be the fuel to be consumed by the artificial intelligence to construct dynamic events to take place in the environment in relationship to the user and the game environment itself. The paper categories the data in second life into abstractions with levels of events occurring in the second life and how to represent all the data in one complete snap shot of the state of the entire system at a time. The events were categorised into primitive, contextual, and complex events. Primitive events were defined as instantaneous and occur at any point in time examples of these are chat, simple player interactions etc. Contextual events is a odd way of introducing a intermediate processing of low-level data abstracted from the virtual world, some examples are static information such as, location data, and position data. Complex events are defined as a set of many events. The idea of these abstractions is to convert them into all into state snap shot of the game which can be interpreted by an AI. The end goal is use this data to implement intelligent virtual agents  with better cognitive abilities. The paper doesn't talk about what kind of AI techniques could be used on this massive data set. In which case the applicable AI that may generate new unique intelligent events may be the use of a genetic algorithm. The algorithm may need to be supervised intially to a point where the offspring of events are corherrent and don't mutate too far from an unrealistic situation. Once a heuristic is applied to the generations then we could have random events in the virtual environment that are not completely random but have somewhat of intelligence and purpose to their creations.
\newpage

%----------------------------------------------------------------------------------------
%	A Learning Strategy for the Autonomous Control of Type 1 Diabetes
%----------------------------------------------------------------------------------------
\section{A Learning Strategy for the Autonomous Control of Type 1 Diabetes} % Sub-section
The applicaiton of artifical intelligence has recently been gaining trust by industry currently being researched and used in the field of medicine. A current issue to be solved today is that millions of people are suffering with Diabetes mellitus. This disease is the inablility of the pancres cells to release insulin in response to levels of glucose in the blood. The problem is a combination of glucose detection and a failure of a biological control system to regulate itself. With current engineering principles of control systems an analogous approach to biolgoical control system using A.I techniques is explored in this paper. Since the sixities there has been considerable research interest for the development of an "artifical pancreas" \cite{Clemens}. The proposed model of the glucose sensor scheme is explained as a continuous blood measurement sensor with the task of the AI is to deal with issues from a continous feedback system. Some examples of this are lag, distortion, calibration errors, filtering and prediction. The fluctionations of the system need to be regulated dynamically in order to provide a consistent insulin release to the blood stream. The proposed control scheme in the paper consists of a inner loop of a feedback controller that regulates the insulin infusion rate as a function of the current derivative of the tracking error in the blood glucose concentration. A HPL block decides whether to interrupt the insulin injector or to limit its rate to a basal value. A MDL block monitors the insulin and blood glucose signals with the purpose of detecting the occurence of meals. The output of the MDL informaton is used by an outer loop to coordinate the controls to a iterative learning algorithm. The iterative learning controller iterativly controls performance from repition to repition. Since the controler if fed a tracking error the goal is to learn how to lower the tracking error as much as possible. Using machine learning is an applied application of artifical intelligence used to lower this error to operational parameters that will allow for the system to learn how to self regulate itself from the feedback provided from bodys blood glucose levels. The results of this model was simulated in MATLAB. The iterative learning scheme was shown to be able to learn using a day by day approach in repsonse to insulin-glucose. Some days, the scheme was able to control insulin administration keeping the blood glucose level in safe ranges. In hindsight will more longterm training simulations the use of machine learning for insulin regulation may be a feasible approach to diabetes patients.
\newpage


%----------------------------------------------------------------------------------------
% EVIDENCE DIRECTED GENERATION OF PLAUSIBLE CRIME SCENARIOS WITH IDENTITY RESOLUTION
%----------------------------------------------------------------------------------------
\section{Evidence Directed Generation Of Plausible Crime Scenarios With Identity Resolution} % Sub-section

The use of artifical intelligence is branching and currently being researched in its application for crime scene investigators to use in order to recreate possible crime scene scenarios given their set of available evidence. The paper defined that its the aim of the investigators and forensic analysts to disclose a scenario that has actually taken place and then determine efficent strategies for proceeding with the investigation. This means that the effectiveness and ability of the investigators to articulate all plausible scenarios in order to identify any course of actions to be implemented in a crime case. Thus the idea is to use artifical intelligence as a tool to construct many variations of crime scene scenarios from a small knownledge base for investigatiors to analyze objectivly and come to differnt conclusions. The representation of evidence in a crime scene under uncertaintly is a difficult task because a criminal might in some situations forge fake evidence to mislead investigators from their path. The conventional apporach to evidence under uncertainty is to represent the evidence using generic relationships between domain objects to their states in a particular scenario, this means that the evidence is essentially stereotyped in order to relate to a certain state in a domain and its objects. This allows the creation of sets of generic crime scenes to be generated. The use of fuzzy logic is applied in this stuation because its parameters may be vauge or undefined varibles. These variable will be transformed into fuzzy sets. The fuzzy sets will contain plausible scenarios which were collected from evidence and a predefined knowledge base. Since the fuzzy logic is useful for instantiating solutions given uncertain parameters it is still up to the investigators to analyze the data and objectivly use the information to come to a crime scene conclusion. The purpose of this is to represent reason in given the information within a reasonable degree of precision. The advantage is that with more information along the process of the investigation, the quality of generated scenarios will increase. In a sense, under supervised learning, the computer and investigator iterate to a solution and eventually approach a mutual human to a computer interface to a solution.


\newpage
%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
%\addcontentsline{toc}{section}{References}
%\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template

\nocite{*}
\bibliographystyle{apacite}
\bibliography{refs}
 
%\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{document}